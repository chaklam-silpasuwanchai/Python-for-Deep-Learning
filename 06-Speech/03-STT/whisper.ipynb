{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)\n",
    "\n",
    "The Whisper architecture is a simple end-to-end approach, implemented as an encoder-decoder Transformer. Input audio is split into 30-second chunks, converted into a log-Mel spectrogram, and then passed into an encoder. A decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation.\n",
    "\n",
    "<img src=\"figures/whisper-arch.png\" title=\"Whisper Framework\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install evaluate\n",
    "# pip install torchaudio\n",
    "# pip install transformers\n",
    "# pip install numpy\n",
    "# pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "from subprocess import CalledProcessError, run\n",
    "from typing import Optional, Union, Dict, Iterable, Optional\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Mel spectrogram\n",
    "\n",
    "A mel spectrogram is a variation of the spectrogram that is commonly used in speech processing and machine learning tasks. It is similar to a spectrogram in that it shows the frequency content of an audio signal over time, but on a different frequency axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_div(x, y):\n",
    "    assert x % y == 0\n",
    "    return x // y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard-coded audio hyperparameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "CHUNK_LENGTH = 30\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n",
    "N_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n",
    "\n",
    "N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n",
    "FRAMES_PER_SECOND = exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n",
    "TOKENS_PER_SECOND = exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This launches a subprocess to decode audio while down-mixing\n",
    "# and resampling as necessary.  Requires the ffmpeg CLI in PATH.\n",
    "# fmt: off\n",
    "    \n",
    "def load_audio(file: str, sr: int = SAMPLE_RATE):\n",
    "    cmd = [\n",
    "        \"ffmpeg\",\n",
    "        \"-nostdin\",\n",
    "        \"-threads\", \"0\",\n",
    "        \"-i\", file,\n",
    "        \"-f\", \"s16le\",\n",
    "        \"-ac\", \"1\",\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ar\", str(sr),\n",
    "        \"-\"\n",
    "    ]\n",
    "    # fmt: on\n",
    "    try:\n",
    "        out = run(cmd, capture_output=True, check=True).stdout\n",
    "    except CalledProcessError as e:\n",
    "        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
    "\n",
    "    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = ''\n",
    "\n",
    "def pad_or_trim(array, length: int = N_SAMPLES, *, axis: int = -1):\n",
    "    \"\"\"\n",
    "    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(array):\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.index_select(\n",
    "                dim=axis, index=torch.arange(length, device=array.device)\n",
    "            )\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n",
    "    else:\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.take(indices=range(length), axis=axis)\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = np.pad(array, pad_widths)\n",
    "\n",
    "    return array\n",
    "\n",
    "def mel_filters(device, n_mels: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    load the mel filterbank matrix for projecting STFT into a Mel spectrogram.\n",
    "    Allows decoupling librosa dependency; saved using:\n",
    "\n",
    "        np.savez_compressed(\n",
    "            \"mel_filters.npz\",\n",
    "            mel_80=librosa.filters.mel(sr=16000, n_fft=400, n_mels=80),\n",
    "            mel_128=librosa.filters.mel(sr=16000, n_fft=400, n_mels=128),\n",
    "        )\n",
    "    \"\"\"\n",
    "    assert n_mels in {80, 128}, f\"Unsupported n_mels: {n_mels}\"\n",
    "\n",
    "    filters_path = os.path.join(os.path.dirname(__file__), \"assets\", \"mel_filters.npz\")\n",
    "    with np.load(filters_path, allow_pickle=False) as f:\n",
    "        return torch.from_numpy(f[f\"mel_{n_mels}\"]).to(device)\n",
    "    \n",
    "# Compute the log-Mel spectrogram of\n",
    "def log_mel_spectrogram(\n",
    "    audio: Union[str, np.ndarray, torch.Tensor],\n",
    "    n_mels: int = 80,\n",
    "    padding: int = 0,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "):\n",
    "    if not torch.is_tensor(audio):\n",
    "        if isinstance(audio, str):\n",
    "            audio = load_audio(audio)\n",
    "        audio = torch.from_numpy(audio)\n",
    "\n",
    "    if device is not None:\n",
    "        audio = audio.to(device)\n",
    "    if padding > 0:\n",
    "        audio = F.pad(audio, (0, padding))\n",
    "    window = torch.hann_window(N_FFT).to(audio.device)\n",
    "    stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)\n",
    "    magnitudes = stft[..., :-1].abs() ** 2\n",
    "\n",
    "    filters = mel_filters(audio.device, n_mels)\n",
    "    mel_spec = filters @ magnitudes\n",
    "\n",
    "    log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n",
    "    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n",
    "    log_spec = (log_spec + 4.0) / 4.0\n",
    "    \n",
    "    return log_spec # torch.Tensor, shape = (80, n_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = load_audio(\"audio.mp3\") #(96597,)\n",
    "audio = pad_or_trim(audio) #(480000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 3000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = log_mel_spectrogram(audio)\n",
    "mel.shape #torch.Size([80, 3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_11_0 (/home/todsavadt/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/th/11.0.0/3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631)\n",
      "Found cached dataset common_voice_11_0 (/home/todsavadt/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/th/11.0.0/3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 42779\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 10930\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_11_0\", \"th\", \n",
    "    split=\"train+validation\", \n",
    "    # use_auth_token=True\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_11_0\", \"th\", \n",
    "    split=\"test\", \n",
    "    # use_auth_token=True\n",
    ")\n",
    "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
    "common_voice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load WhisperFeatureExtractor\n",
    "- Padding and spectrogram conversion are both handled by the Transformers Whisper feature extractor in a single line of code! \n",
    "- To prepare for our audio data, letâ€™s now load the feature extractor from the pre-trained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load WhisperTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"Thai\",  task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_str = common_voice[\"train\"][0][\"sentence\"]\n",
    "# labels = tokenizer(input_str).input_ids\n",
    "# decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "# decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# print(f\"Input:                 {input_str}\")\n",
    "# print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "# print(f\"Decoded w/out special: {decoded_str}\")\n",
    "# print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine To Create A WhisperProcessor\n",
    "\n",
    "We can combine the tokenizer and feature extractor into a single WhisperProcessor class to make using them easier. \n",
    "This processor object can be applied to audio inputs and model predictions as necessary and derives from the WhisperFeatureExtractor and WhisperProcessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"Thai\",  task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "Now we can write a function to prepare our data ready for the model:\n",
    "\n",
    "1. Using the batch[audio] function, we load and resample the audio data. Datasets, as previously mentioned, carry out any necessary resampling operations in real time.\n",
    "2. From our 1-dimensional audio array, we compute the log-Mel spectrogram input features using the feature extractor.\n",
    "3. Using the tokenizer, we encode the transcriptions to create label ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(examples):\n",
    "    # compute log-Mel input features from input audio array\n",
    "    audio = examples[\"audio\"]\n",
    "    examples[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "\n",
    "    del examples[\"audio\"]\n",
    "    sentences = examples[\"sentence\"]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    examples[\"labels\"] = tokenizer(sentences).input_ids\n",
    "    del examples[\"sentence\"]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/todsavadt/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/th/11.0.0/3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631/cache-4e725f4090f2c033.arrow\n",
      "Loading cached shuffled indices for dataset at /home/todsavadt/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/th/11.0.0/3f27acf10f303eac5b6fbbbe02495aeddb46ecffdb0a2fe3507fcfbf89094631/cache-b5ea01e9bfcfb160.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset_ = common_voice['train'].shuffle(seed=55).select(list(range(1000)))\n",
    "test_dataset_  = common_voice['test'].shuffle(seed=55).select(list(range(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b1fbfcc8fa46a9b080e10d353ee051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6701ba6f0244f1cbc9098c18fb9f50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset_.map(prepare_dataset)\n",
    "test_dataset  = test_dataset_.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        # labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), tokenizer.pad_token_id)\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "## initiate the data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "eval_dataloader  = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "<img src=\"figures/asr-summary-of-model-architecture-desktop.svg\" title=\"Whisper Framework\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight.to(x.dtype), None if self.bias is None else self.bias.to(x.dtype),)\n",
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(self, x, weight, bias):\n",
    "        return super()._conv_forward(x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype))\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state, n_head):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query  = Linear(n_state, n_state)\n",
    "        self.key    = Linear(n_state, n_state, bias=False)\n",
    "        self.value  = Linear(n_state, n_state)\n",
    "        self.out    = Linear(n_state, n_state)\n",
    "\n",
    "    def forward(self, x, xa = None, mask = None, kv_cache  = None, ):\n",
    "        q = self.query(x)\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
    "            # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(self, q , k , v , mask = None):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk = q @ k\n",
    "        if mask is not None:\n",
    "            qk = qk + mask[:n_ctx, :n_ctx]\n",
    "        qk = qk.float()\n",
    "        w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state, n_head, cross_attention = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x, xa = None, mask = None, kv_cache = None,):\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x : torch.Tensor, shape = (batch_size, n_mels, n_ctx) the mel spectrogram of the audio\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
    "        x = (x + self.positional_embedding).to(x.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDecoder(nn.Module):\n",
    "    def __init__(self, n_vocab, n_ctx, n_state, n_head, n_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_state)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [\n",
    "                ResidualAttentionBlock(n_state, n_head, cross_attention=True)\n",
    "                for _ in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "        self.ln = LayerNorm(n_state)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self, x, xa, kv_cache = None):\n",
    "        # x = (batch_size, <= n_ctx)\n",
    "        # xa = (batch_size, n_audio_ctx, n_audio_state)  \n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        x = (\n",
    "            self.token_embedding(x)\n",
    "            + self.positional_embedding[offset : offset + x.shape[-1]]\n",
    "        )\n",
    "        x = x.to(xa.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
    "\n",
    "        x = self.ln(x)\n",
    "        logits = (x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)).float()\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import gzip\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelDimensions:\n",
    "    n_mels: int\n",
    "    n_audio_ctx: int\n",
    "    n_audio_state: int\n",
    "    n_audio_head: int\n",
    "    n_audio_layer: int\n",
    "    n_vocab: int\n",
    "    n_text_ctx: int\n",
    "    n_text_state: int\n",
    "    n_text_head: int\n",
    "    n_text_layer: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Whisper(nn.Module):\n",
    "    def __init__(self, dims: ModelDimensions):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.dims.n_mels,\n",
    "            self.dims.n_audio_ctx,\n",
    "            self.dims.n_audio_state,\n",
    "            self.dims.n_audio_head,\n",
    "            self.dims.n_audio_layer,\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.dims.n_vocab,\n",
    "            self.dims.n_text_ctx,\n",
    "            self.dims.n_text_state,\n",
    "            self.dims.n_text_head,\n",
    "            self.dims.n_text_layer,\n",
    "        )\n",
    "        # use the last half among the decoder layers for time alignment by default;\n",
    "        # to use a specific set of heads, see `set_alignment_heads()` below.\n",
    "        all_heads = torch.zeros(\n",
    "            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.dims.n_text_layer // 2 :] = True\n",
    "        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, mel):\n",
    "        return self.encoder(mel)\n",
    "\n",
    "    def logits(self, tokens, audio_features):\n",
    "        return self.decoder(tokens, audio_features)\n",
    "\n",
    "    def forward(self, mel, tokens):\n",
    "        audio_encoder = self.encoder(mel)\n",
    "        text_decoder  = self.decoder(tokens, audio_encoder)\n",
    "        return text_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = ModelDimensions(\n",
    "    n_mels=80, \n",
    "    n_audio_ctx=1500, \n",
    "    n_audio_state=384, \n",
    "    n_audio_head=6, \n",
    "    n_audio_layer=4, \n",
    "    n_vocab=51865, \n",
    "    n_text_ctx=448, \n",
    "    n_text_state=384, \n",
    "    n_text_head=6, \n",
    "    n_text_layer=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 384)\n",
       "    (blocks): ModuleList(\n",
       "      (0-3): 4 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (key): Linear(in_features=384, out_features=384, bias=False)\n",
       "          (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (out): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Whisper(dims)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todsavadt/.local/lib/python3.10/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "model_openai = whisper.load_model(\"tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.token_embedding = model_openai.decoder.token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92160\n",
      "   384\n",
      "442368\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "589824\n",
      "  1536\n",
      "589824\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "589824\n",
      "  1536\n",
      "589824\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "589824\n",
      "  1536\n",
      "589824\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "589824\n",
      "  1536\n",
      "589824\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "172032\n",
      "19916160\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "589824\n",
      "  1536\n",
      "589824\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "589824\n",
      "  1536\n",
      "589824\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "589824\n",
      "  1536\n",
      "589824\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "147456\n",
      "   384\n",
      "147456\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "589824\n",
      "  1536\n",
      "589824\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "   384\n",
      "______\n",
      "37184640\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 80, 3000]), torch.Size([16, 42]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "\n",
    "batch['input_features'].shape, batch['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 42, 51865])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(batch['input_features'].to(device), batch['labels'].to(device))\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/asr-details-desktop.svg\" title=\"Whisper Framework\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0ee678e3d44367be855e8d64edf1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 56s\n",
      "Train Loss: 4.547\n",
      "WER: 1.000\n",
      "Epoch: 02 | Time: 0m 56s\n",
      "Train Loss: 1.536\n",
      "WER: 0.984\n",
      "Epoch: 03 | Time: 0m 56s\n",
      "Train Loss: 0.259\n",
      "WER: 0.814\n",
      "Epoch: 04 | Time: 0m 56s\n",
      "Train Loss: 0.025\n",
      "WER: 4.767\n",
      "Epoch: 05 | Time: 0m 56s\n",
      "Train Loss: 0.006\n",
      "WER: 19.271\n",
      "Epoch: 06 | Time: 0m 56s\n",
      "Train Loss: 0.002\n",
      "WER: 0.829\n",
      "Epoch: 07 | Time: 0m 56s\n",
      "Train Loss: 0.001\n",
      "WER: 2.372\n",
      "Epoch: 08 | Time: 0m 56s\n",
      "Train Loss: 0.000\n",
      "WER: 3.031\n",
      "Epoch: 09 | Time: 0m 56s\n",
      "Train Loss: 0.000\n",
      "WER: 1.178\n",
      "Epoch: 10 | Time: 0m 55s\n",
      "Train Loss: 0.000\n",
      "WER: 6.395\n"
     ]
    }
   ],
   "source": [
    "clip = 0.001\n",
    "train_losses = []\n",
    "wer_scores = []  # List to store WER scores per epoch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Train\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "    \n",
    "        mel = batch['input_features'].to(device)\n",
    "        tokens = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(mel, tokens)\n",
    "        loss = criterion(logits.view(-1, dims.n_vocab), tokens.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        # Clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "            \n",
    "        epoch_loss += loss.item() \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Calculate the average loss for the epoch\n",
    "    avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Evaluate\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    model.eval()\n",
    "    epoch_wer = 0\n",
    "    for step, batch in enumerate(eval_dataloader):  # Use enumerate here\n",
    "        with torch.no_grad():\n",
    "            mel = batch['input_features'].to(device)\n",
    "            tokens = batch['labels'].to(device)\n",
    "    \n",
    "            logits = model(mel, tokens)\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "    \n",
    "            # Convert token indices to strings\n",
    "            predictions_text = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            references_text = tokenizer.batch_decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "            wer_metric.add_batch(predictions=predictions_text, references=references_text)\n",
    "        \n",
    "    avg_wer = wer_metric.compute()\n",
    "\n",
    "    # Calculate the average WER for the epoch using step+1 for total batches\n",
    "    wer_scores.append(avg_wer)  # Store average WER in the list\n",
    "            \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.3f}\")\n",
    "    print(f\"WER: {avg_wer:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "# ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open-AI Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U openai-whisper\n",
    "# !pip install torchaudio\n",
    "# !pip install jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the LibriSpeech dataset\n",
    "\n",
    "The following will load the test-clean split of the LibriSpeech corpus using torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeech(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple class to wrap LibriSpeech and trim/pad the audio to 30 seconds.\n",
    "    It will drop the last few seconds of a very small portion of the utterances.\n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"test-clean\", device=device):\n",
    "        self.dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "            root=os.path.expanduser(\"~/.cache\"),\n",
    "            url=split,\n",
    "            download=True,\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
    "        assert sample_rate == 16000\n",
    "        audio = whisper.pad_or_trim(audio.flatten()).to(self.device)\n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        \n",
    "        return (mel, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LibriSpeech(\"test-clean\")\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todsavadt/.local/lib/python3.10/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Help me in here! Help me in here!\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"tiny\")\n",
    "result = model.transcribe(\"audio.mp3\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on the dataset using a base Whisper model\n",
    "\n",
    "The following will take a few minutes to transcribe all utterances in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is multilingual and has 37,184,640 parameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/todsavadt/.local/lib/python3.10/site-packages/whisper/__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"tiny\")\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict without timestamps for short-form transcription\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aafa5abdaf34eaa93adf5a222f88f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hypotheses = []\n",
    "references = []\n",
    "\n",
    "for mels, texts in tqdm(loader):\n",
    "    results = model.decode(mels, options)\n",
    "    hypotheses.extend([result.text for result in results])\n",
    "    references.extend(texts)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He hoped there would be stew for dinner, turni...</td>\n",
       "      <td>HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stuffed into you, his belly, couchled him.</td>\n",
       "      <td>STUFF IT INTO YOU HIS BELLY COUNSELLED HIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After early nightfall, the yellow lamps would ...</td>\n",
       "      <td>AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey Bertie, any good in your mind?</td>\n",
       "      <td>HELLO BERTIE ANY GOOD IN YOUR MIND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number 10 Fresh Nelly is waiting on you. Good ...</td>\n",
       "      <td>NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The music came nearer and he recalled the word...</td>\n",
       "      <td>THE MUSIC CAME NEARER AND HE RECALLED THE WORD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The dull light fell more faintly upon the page...</td>\n",
       "      <td>THE DULL LIGHT FELL MORE FAINTLY UPON THE PAGE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A cold lucid indifference rained in his soul.</td>\n",
       "      <td>A COLD LUCID INDIFFERENCE REIGNED IN HIS SOUL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The chaos in which his order extinguished itse...</td>\n",
       "      <td>THE CHAOS IN WHICH HIS ARDOUR EXTINGUISHED ITS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>At most, by an arms given to a beggar whose bl...</td>\n",
       "      <td>AT MOST BY AN ALMS GIVEN TO A BEGGAR WHOSE BLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Well now, in this I declare you have a head an...</td>\n",
       "      <td>WELL NOW ENNIS I DECLARE YOU HAVE A HEAD AND S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>On Saturday mornings when the so-dality met in...</td>\n",
       "      <td>ON SATURDAY MORNINGS WHEN THE SODALITY MET IN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Her eyes seemed to regard him with mild pity. ...</td>\n",
       "      <td>HER EYES SEEMED TO REGARD HIM WITH MILD PITY H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>If ever he was impelled to cast sin from him a...</td>\n",
       "      <td>IF EVER HE WAS IMPELLED TO CAST SIN FROM HIM A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>He tried to think how it could be.</td>\n",
       "      <td>HE TRIED TO THINK HOW IT COULD BE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>but the dusk deepening in the school room cove...</td>\n",
       "      <td>BUT THE DUSK DEEPENING IN THE SCHOOLROOM COVER...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           hypothesis  \\\n",
       "0   He hoped there would be stew for dinner, turni...   \n",
       "1          Stuffed into you, his belly, couchled him.   \n",
       "2   After early nightfall, the yellow lamps would ...   \n",
       "3                  Hey Bertie, any good in your mind?   \n",
       "4   Number 10 Fresh Nelly is waiting on you. Good ...   \n",
       "5   The music came nearer and he recalled the word...   \n",
       "6   The dull light fell more faintly upon the page...   \n",
       "7       A cold lucid indifference rained in his soul.   \n",
       "8   The chaos in which his order extinguished itse...   \n",
       "9   At most, by an arms given to a beggar whose bl...   \n",
       "10  Well now, in this I declare you have a head an...   \n",
       "11  On Saturday mornings when the so-dality met in...   \n",
       "12  Her eyes seemed to regard him with mild pity. ...   \n",
       "13  If ever he was impelled to cast sin from him a...   \n",
       "14                 He tried to think how it could be.   \n",
       "15  but the dusk deepening in the school room cove...   \n",
       "\n",
       "                                            reference  \n",
       "0   HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...  \n",
       "1          STUFF IT INTO YOU HIS BELLY COUNSELLED HIM  \n",
       "2   AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...  \n",
       "3                  HELLO BERTIE ANY GOOD IN YOUR MIND  \n",
       "4   NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...  \n",
       "5   THE MUSIC CAME NEARER AND HE RECALLED THE WORD...  \n",
       "6   THE DULL LIGHT FELL MORE FAINTLY UPON THE PAGE...  \n",
       "7       A COLD LUCID INDIFFERENCE REIGNED IN HIS SOUL  \n",
       "8   THE CHAOS IN WHICH HIS ARDOUR EXTINGUISHED ITS...  \n",
       "9   AT MOST BY AN ALMS GIVEN TO A BEGGAR WHOSE BLE...  \n",
       "10  WELL NOW ENNIS I DECLARE YOU HAVE A HEAD AND S...  \n",
       "11  ON SATURDAY MORNINGS WHEN THE SODALITY MET IN ...  \n",
       "12  HER EYES SEEMED TO REGARD HIM WITH MILD PITY H...  \n",
       "13  IF EVER HE WAS IMPELLED TO CAST SIN FROM HIM A...  \n",
       "14                  HE TRIED TO THINK HOW IT COULD BE  \n",
       "15  BUT THE DUSK DEEPENING IN THE SCHOOLROOM COVER...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the word error rate\n",
    "\n",
    "Now, we use our English normalizer implementation to standardize the transcription and calculate the WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "\n",
    "normalizer = EnglishTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>reference</th>\n",
       "      <th>hypothesis_clean</th>\n",
       "      <th>reference_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He hoped there would be stew for dinner, turni...</td>\n",
       "      <td>HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...</td>\n",
       "      <td>he hoped there would be stew for dinner turnip...</td>\n",
       "      <td>he hoped there would be stew for dinner turnip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stuffed into you, his belly, couchled him.</td>\n",
       "      <td>STUFF IT INTO YOU HIS BELLY COUNSELLED HIM</td>\n",
       "      <td>stuffed into you his belly couchled him</td>\n",
       "      <td>stuff it into you his belly counseled him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After early nightfall, the yellow lamps would ...</td>\n",
       "      <td>AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...</td>\n",
       "      <td>after early nightfall the yellow lamps would l...</td>\n",
       "      <td>after early nightfall the yellow lamps would l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hey Bertie, any good in your mind?</td>\n",
       "      <td>HELLO BERTIE ANY GOOD IN YOUR MIND</td>\n",
       "      <td>hey bertie any good in your mind</td>\n",
       "      <td>hello bertie any good in your mind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number 10 Fresh Nelly is waiting on you. Good ...</td>\n",
       "      <td>NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...</td>\n",
       "      <td>number 10 fresh nelly is waiting on you good n...</td>\n",
       "      <td>number 10 fresh nelly is waiting on you good n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The music came nearer and he recalled the word...</td>\n",
       "      <td>THE MUSIC CAME NEARER AND HE RECALLED THE WORD...</td>\n",
       "      <td>the music came nearer and he recalled the word...</td>\n",
       "      <td>the music came nearer and he recalled the word...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The dull light fell more faintly upon the page...</td>\n",
       "      <td>THE DULL LIGHT FELL MORE FAINTLY UPON THE PAGE...</td>\n",
       "      <td>the dull light fell more faintly upon the page...</td>\n",
       "      <td>the dull light fell more faintly upon the page...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A cold lucid indifference rained in his soul.</td>\n",
       "      <td>A COLD LUCID INDIFFERENCE REIGNED IN HIS SOUL</td>\n",
       "      <td>a cold lucid indifference rained in his soul</td>\n",
       "      <td>a cold lucid indifference reigned in his soul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The chaos in which his order extinguished itse...</td>\n",
       "      <td>THE CHAOS IN WHICH HIS ARDOUR EXTINGUISHED ITS...</td>\n",
       "      <td>the chaos in which his order extinguished itse...</td>\n",
       "      <td>the chaos in which his ardor extinguished itse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>At most, by an arms given to a beggar whose bl...</td>\n",
       "      <td>AT MOST BY AN ALMS GIVEN TO A BEGGAR WHOSE BLE...</td>\n",
       "      <td>at most by an arms given to a beggar whose ble...</td>\n",
       "      <td>at most by an alms given to a beggar whose ble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Well now, in this I declare you have a head an...</td>\n",
       "      <td>WELL NOW ENNIS I DECLARE YOU HAVE A HEAD AND S...</td>\n",
       "      <td>well now in this i declare you have a head and...</td>\n",
       "      <td>well now ennis i declare you have a head and s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>On Saturday mornings when the so-dality met in...</td>\n",
       "      <td>ON SATURDAY MORNINGS WHEN THE SODALITY MET IN ...</td>\n",
       "      <td>on saturday mornings when the so dality met in...</td>\n",
       "      <td>on saturday mornings when the sodality met in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Her eyes seemed to regard him with mild pity. ...</td>\n",
       "      <td>HER EYES SEEMED TO REGARD HIM WITH MILD PITY H...</td>\n",
       "      <td>her eyes seemed to regard him with mild pity h...</td>\n",
       "      <td>her eyes seemed to regard him with mild pity h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>If ever he was impelled to cast sin from him a...</td>\n",
       "      <td>IF EVER HE WAS IMPELLED TO CAST SIN FROM HIM A...</td>\n",
       "      <td>if ever he was impelled to cast sin from him a...</td>\n",
       "      <td>if ever he was impelled to cast sin from him a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>He tried to think how it could be.</td>\n",
       "      <td>HE TRIED TO THINK HOW IT COULD BE</td>\n",
       "      <td>he tried to think how it could be</td>\n",
       "      <td>he tried to think how it could be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>but the dusk deepening in the school room cove...</td>\n",
       "      <td>BUT THE DUSK DEEPENING IN THE SCHOOLROOM COVER...</td>\n",
       "      <td>but the dusk deepening in the school room cove...</td>\n",
       "      <td>but the dusk deepening in the schoolroom cover...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           hypothesis  \\\n",
       "0   He hoped there would be stew for dinner, turni...   \n",
       "1          Stuffed into you, his belly, couchled him.   \n",
       "2   After early nightfall, the yellow lamps would ...   \n",
       "3                  Hey Bertie, any good in your mind?   \n",
       "4   Number 10 Fresh Nelly is waiting on you. Good ...   \n",
       "5   The music came nearer and he recalled the word...   \n",
       "6   The dull light fell more faintly upon the page...   \n",
       "7       A cold lucid indifference rained in his soul.   \n",
       "8   The chaos in which his order extinguished itse...   \n",
       "9   At most, by an arms given to a beggar whose bl...   \n",
       "10  Well now, in this I declare you have a head an...   \n",
       "11  On Saturday mornings when the so-dality met in...   \n",
       "12  Her eyes seemed to regard him with mild pity. ...   \n",
       "13  If ever he was impelled to cast sin from him a...   \n",
       "14                 He tried to think how it could be.   \n",
       "15  but the dusk deepening in the school room cove...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   HE HOPED THERE WOULD BE STEW FOR DINNER TURNIP...   \n",
       "1          STUFF IT INTO YOU HIS BELLY COUNSELLED HIM   \n",
       "2   AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD L...   \n",
       "3                  HELLO BERTIE ANY GOOD IN YOUR MIND   \n",
       "4   NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD ...   \n",
       "5   THE MUSIC CAME NEARER AND HE RECALLED THE WORD...   \n",
       "6   THE DULL LIGHT FELL MORE FAINTLY UPON THE PAGE...   \n",
       "7       A COLD LUCID INDIFFERENCE REIGNED IN HIS SOUL   \n",
       "8   THE CHAOS IN WHICH HIS ARDOUR EXTINGUISHED ITS...   \n",
       "9   AT MOST BY AN ALMS GIVEN TO A BEGGAR WHOSE BLE...   \n",
       "10  WELL NOW ENNIS I DECLARE YOU HAVE A HEAD AND S...   \n",
       "11  ON SATURDAY MORNINGS WHEN THE SODALITY MET IN ...   \n",
       "12  HER EYES SEEMED TO REGARD HIM WITH MILD PITY H...   \n",
       "13  IF EVER HE WAS IMPELLED TO CAST SIN FROM HIM A...   \n",
       "14                  HE TRIED TO THINK HOW IT COULD BE   \n",
       "15  BUT THE DUSK DEEPENING IN THE SCHOOLROOM COVER...   \n",
       "\n",
       "                                     hypothesis_clean  \\\n",
       "0   he hoped there would be stew for dinner turnip...   \n",
       "1             stuffed into you his belly couchled him   \n",
       "2   after early nightfall the yellow lamps would l...   \n",
       "3                    hey bertie any good in your mind   \n",
       "4   number 10 fresh nelly is waiting on you good n...   \n",
       "5   the music came nearer and he recalled the word...   \n",
       "6   the dull light fell more faintly upon the page...   \n",
       "7        a cold lucid indifference rained in his soul   \n",
       "8   the chaos in which his order extinguished itse...   \n",
       "9   at most by an arms given to a beggar whose ble...   \n",
       "10  well now in this i declare you have a head and...   \n",
       "11  on saturday mornings when the so dality met in...   \n",
       "12  her eyes seemed to regard him with mild pity h...   \n",
       "13  if ever he was impelled to cast sin from him a...   \n",
       "14                  he tried to think how it could be   \n",
       "15  but the dusk deepening in the school room cove...   \n",
       "\n",
       "                                      reference_clean  \n",
       "0   he hoped there would be stew for dinner turnip...  \n",
       "1           stuff it into you his belly counseled him  \n",
       "2   after early nightfall the yellow lamps would l...  \n",
       "3                  hello bertie any good in your mind  \n",
       "4   number 10 fresh nelly is waiting on you good n...  \n",
       "5   the music came nearer and he recalled the word...  \n",
       "6   the dull light fell more faintly upon the page...  \n",
       "7       a cold lucid indifference reigned in his soul  \n",
       "8   the chaos in which his ardor extinguished itse...  \n",
       "9   at most by an alms given to a beggar whose ble...  \n",
       "10  well now ennis i declare you have a head and s...  \n",
       "11  on saturday mornings when the sodality met in ...  \n",
       "12  her eyes seemed to regard him with mild pity h...  \n",
       "13  if ever he was impelled to cast sin from him a...  \n",
       "14                  he tried to think how it could be  \n",
       "15  but the dusk deepening in the schoolroom cover...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"hypothesis_clean\"] = [normalizer(text) for text in data[\"hypothesis\"]]\n",
    "data[\"reference_clean\"] = [normalizer(text) for text in data[\"reference\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 7.41 %\n"
     ]
    }
   ],
   "source": [
    "wer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "\n",
    "print(f\"WER: {wer * 100:.2f} %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
