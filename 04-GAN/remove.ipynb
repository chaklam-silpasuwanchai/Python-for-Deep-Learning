{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GAN)\n",
    "\n",
    "This is an implementation of [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661).\n",
    "\n",
    "The generator, $G(\\pmb{z}; \\theta_g)$ generates samples that match the\n",
    "distribution of data, while the discriminator, $D(\\pmb{x}; \\theta_g)$\n",
    "gives the probability that $\\pmb{x}$ came from data rather than $G$.\n",
    "\n",
    "We train $D$ and $G$ simultaneously on a two-player min-max game with value\n",
    "function $V(G, D)$.\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) =\n",
    "    \\mathop{\\mathbb{E}}_{\\pmb{x} \\sim p_{data}(\\pmb{x})}\n",
    "        \\big[\\log D(\\pmb{x})\\big] +\n",
    "    \\mathop{\\mathbb{E}}_{\\pmb{z} \\sim p_{\\pmb{z}}(\\pmb{z})}\n",
    "        \\big[\\log (1 - D(G(\\pmb{z}))\\big]\n",
    "$$\n",
    "\n",
    "$p_{data}(\\pmb{x})$ is the probability distribution over data,\n",
    "whilst $p_{\\pmb{z}}(\\pmb{z})$ probability distribution of $\\pmb{z}$, which is set to\n",
    "gaussian noise.\n",
    "\n",
    "This file defines the loss functions. [Here](experiment.html) is an MNIST example\n",
    "with two multilayer perceptron for the generator and discriminator.\n",
    "\n",
    "This part of the lab is adapted from [labmlai's annotated_deep_learning_paper_implementations repository on GitHub](https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "\n",
    "class DiscriminatorLogitsLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ## Discriminator Loss\n",
    "\n",
    "    Discriminator should **ascend** on the gradient,\n",
    "\n",
    "    $$\\nabla_{\\theta_d} \\frac{1}{m} \\sum_{i=1}^m \\Bigg[\n",
    "        \\log D\\Big(\\pmb{x}^{(i)}\\Big) +\n",
    "        \\log \\Big(1 - D\\Big(G\\Big(\\pmb{z}^{(i)}\\Big)\\Big)\\Big)\n",
    "    \\Bigg]$$\n",
    "\n",
    "    $m$ is the mini-batch size and $(i)$ is used to index samples in the mini-batch.\n",
    "    $\\pmb{x}$ are samples from $p_{data}$ and $\\pmb{z}$ are samples from $p_z$.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, smoothing: float = 0.2):\n",
    "        super().__init__()\n",
    "        # We use PyTorch Binary Cross Entropy Loss, which is\n",
    "        # $-\\sum\\Big[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})\\Big]$,\n",
    "        # where $y$ are the labels and $\\hat{y}$ are the predictions.\n",
    "        # *Note the negative sign*.\n",
    "        # We use labels equal to $1$ for $\\pmb{x}$ from $p_{data}$\n",
    "        # and labels equal to $0$ for $\\pmb{x}$ from $p_{G}.$\n",
    "        # Then descending on the sum of these is the same as ascending on\n",
    "        # the above gradient.\n",
    "        #\n",
    "        # `BCEWithLogitsLoss` combines softmax and binary cross entropy loss.\n",
    "        self.loss_true = nn.BCEWithLogitsLoss()\n",
    "        self.loss_false = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # We use label smoothing because it seems to work better in some cases\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        # Labels are registered as buffered and persistence is set to `False`.\n",
    "        self.register_buffer('labels_true', _create_labels(256, 1.0 - smoothing, 1.0), False)\n",
    "        self.register_buffer('labels_false', _create_labels(256, 0.0, smoothing), False)\n",
    "\n",
    "    def forward(self, logits_true: torch.Tensor, logits_false: torch.Tensor):\n",
    "        \"\"\"\n",
    "        `logits_true` are logits from $D(\\pmb{x}^{(i)})$ and\n",
    "        `logits_false` are logits from $D(G(\\pmb{z}^{(i)}))$\n",
    "        \"\"\"\n",
    "        if len(logits_true) > len(self.labels_true):\n",
    "            self.register_buffer(\"labels_true\",\n",
    "                                 _create_labels(len(logits_true), 1.0 - self.smoothing, 1.0, logits_true.device), False)\n",
    "        if len(logits_false) > len(self.labels_false):\n",
    "            self.register_buffer(\"labels_false\",\n",
    "                                 _create_labels(len(logits_false), 0.0, self.smoothing, logits_false.device), False)\n",
    "\n",
    "        return (self.loss_true(logits_true, self.labels_true[:len(logits_true)]),\n",
    "                self.loss_false(logits_false, self.labels_false[:len(logits_false)]))\n",
    "\n",
    "\n",
    "class GeneratorLogitsLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ## Generator Loss\n",
    "\n",
    "    Generator should **descend** on the gradient,\n",
    "\n",
    "    $$\\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^m \\Bigg[\n",
    "        \\log \\Big(1 - D\\Big(G\\Big(\\pmb{z}^{(i)}\\Big)\\Big)\\Big)\n",
    "    \\Bigg]$$\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.loss_true = nn.BCEWithLogitsLoss()\n",
    "        self.smoothing = smoothing\n",
    "        # We use labels equal to $1$ for $\\pmb{x}$ from $p_{G}.$\n",
    "        # Then descending on this loss is the same as descending on\n",
    "        # the above gradient.\n",
    "        self.register_buffer('fake_labels', _create_labels(256, 1.0 - smoothing, 1.0), False)\n",
    "\n",
    "    def forward(self, logits: torch.Tensor):\n",
    "        if len(logits) > len(self.fake_labels):\n",
    "            self.register_buffer(\"fake_labels\",\n",
    "                                 _create_labels(len(logits), 1.0 - self.smoothing, 1.0, logits.device), False)\n",
    "\n",
    "        return self.loss_true(logits, self.fake_labels[:len(logits)])\n",
    "\n",
    "\n",
    "def _create_labels(n: int, r1: float, r2: float, device: torch.device = None):\n",
    "    \"\"\"\n",
    "    Create smoothed labels\n",
    "    \"\"\"\n",
    "    return torch.empty(n, 1, requires_grad=False, device=device).uniform_(r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Simple MLP Generator\n",
    "\n",
    "    This has three linear layers of increasing size with `LeakyReLU` activations.\n",
    "    The final layer has a $tanh$ activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer_sizes = [256, 512, 1024]\n",
    "        layers = []\n",
    "        d_prev = 100\n",
    "        for size in layer_sizes:\n",
    "            layers = layers + [nn.Linear(d_prev, size), nn.LeakyReLU(0.2)]\n",
    "            d_prev = size\n",
    "\n",
    "        self.layers = nn.Sequential(*layers, nn.Linear(d_prev, 28 * 28), nn.Tanh())\n",
    "\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x).view(x.shape[0], 1, 28, 28)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Simple MLP Discriminator\n",
    "\n",
    "    This has three  linear layers of decreasing size with `LeakyReLU` activations.\n",
    "    The final layer has a single output that gives the logit of whether input\n",
    "    is real or fake. You can get the probability by calculating the sigmoid of it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layer_sizes = [1024, 512, 256]\n",
    "        layers = []\n",
    "        d_prev = 28 * 28\n",
    "        for size in layer_sizes:\n",
    "            layers = layers + [nn.Linear(d_prev, size), nn.LeakyReLU(0.2)]\n",
    "            d_prev = size\n",
    "\n",
    "        self.layers = nn.Sequential(*layers, nn.Linear(d_prev, 1))\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x.view(x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
