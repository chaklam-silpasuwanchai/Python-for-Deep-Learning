{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "\n",
    "In this lab, we will develop several basic GANs and experiment with them.\n",
    "\n",
    "Some of the information in this lab is based on material from Building Basic Generative Adversarial Networks (GANs) in Coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models vs Discriminative Models\n",
    "\n",
    "Discriminative models are typically used for classification in machine learning.\n",
    "Discriminative classifiers take a set of features $x$, such as having a nose or wheels,\n",
    "and from these features determine a category $y$, \n",
    "meaning that they try to model the probability of class $y$ given the set of features $x$.\n",
    "Assuming $X$ is a random variable over sets of features and $Y$ is a random variable over sets of possible classes,\n",
    "a discriminative models estimates\n",
    "\n",
    "$$D(x) = P(Y=y \\mid X=x).$$\n",
    "\n",
    "Generative models, however, model $P(x)$ or $P(x \\mid y)$.\n",
    "Generative models based on sampling take a random input and sometimes also a class $y$ such as a \"dog.\"\n",
    "From these inputs, a generative sampler will attempt to generate a set of features $x$ that are\n",
    "representative of the class \"dog.\" The random noise input ensures that we don't generate the same\n",
    "dog each time.\n",
    "\n",
    "Assuming random noise distribution $N$, a conditional sample-based generative model attempts\n",
    "the following:\n",
    "\n",
    "1. Input class $y$\n",
    "2. $z \\sim N$\n",
    "3. $x = G(z,y)$\n",
    "\n",
    "the goal is $P_{z\\sim N}(G(z,y)=x) = P(x \\mid y)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANS)\n",
    "\n",
    "GANs for images are composed of two models, a generator that generates images\n",
    "and a discriminator that is a discriminative classifier.\n",
    "The generator takes in a random noise input and an optional class and\n",
    "deterministically transforms the input into an image. The discriminator attempts to \n",
    "determine which of its inputs are real samples from the data distribution and which ones\n",
    "are fake samples generated by the generator. Over time, the models compete. If the training is\n",
    "set up well, when complete, the generator can take in any random noise input and produce a realistic result.\n",
    "In summary, $G$ learns to produce realistic examples like an artist painting paintings that look like photos,\n",
    "while $D$ distinguishes the painted photos from real photos.\n",
    "The basic GAN model described by Goodfellow et al. (2014) looks like this:\n",
    "\n",
    "<img src=\"figures/gan_architecture-1.png\" title=\"GAN Framework\" style=\"width: 640px;\" />\n",
    "\n",
    "After this lab, you may be interested in reading about [6 GAN Architectures You Really Should Know](https://neptune.ai/blog/6-gan-architectures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gan Setup\n",
    "\n",
    "Let's build our first GAN.\n",
    "\n",
    "There are about a million tutorials on coding GANs with PyTorch available online. We'll use\n",
    "[code from GitHub user diegoalejogm](https://github.com/diegoalejogm/gans).\n",
    "\n",
    "To run this code you'll need some dependencies such as tensorboardX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a `Logger` class with useful tricks to indicate training progress and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import errno\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "'''\n",
    "    TensorBoard Data will be stored in './runs' path\n",
    "'''\n",
    "\n",
    "\n",
    "class Logger:\n",
    "\n",
    "    def __init__(self, model_name, data_name):\n",
    "        self.model_name = model_name\n",
    "        self.data_name = data_name\n",
    "\n",
    "        self.comment = '{}_{}'.format(model_name, data_name)\n",
    "        self.data_subdir = '{}/{}'.format(model_name, data_name)\n",
    "\n",
    "        # TensorBoard\n",
    "        self.writer = SummaryWriter(comment=self.comment)\n",
    "\n",
    "    def log(self, d_error, g_error, epoch, n_batch, num_batches):\n",
    "\n",
    "        # var_class = torch.autograd.variable.Variable\n",
    "        if isinstance(d_error, torch.autograd.Variable):\n",
    "            d_error = d_error.data.cpu().numpy()\n",
    "        if isinstance(g_error, torch.autograd.Variable):\n",
    "            g_error = g_error.data.cpu().numpy()\n",
    "\n",
    "        step = Logger._step(epoch, n_batch, num_batches)\n",
    "        self.writer.add_scalar(\n",
    "            '{}/D_error'.format(self.comment), d_error, step)\n",
    "        self.writer.add_scalar(\n",
    "            '{}/G_error'.format(self.comment), g_error, step)\n",
    "\n",
    "    def log_images(self, images, num_images, epoch, n_batch, num_batches, format='NCHW', normalize=True):\n",
    "        '''\n",
    "        input images are expected in format (NCHW)\n",
    "        '''\n",
    "        if type(images) == np.ndarray:\n",
    "            images = torch.from_numpy(images)\n",
    "        \n",
    "        if format=='NHWC':\n",
    "            images = images.transpose(1,3)\n",
    "        \n",
    "\n",
    "        step = Logger._step(epoch, n_batch, num_batches)\n",
    "        img_name = '{}/images{}'.format(self.comment, '')\n",
    "\n",
    "        # Make horizontal grid from image tensor\n",
    "        horizontal_grid = vutils.make_grid(\n",
    "            images, normalize=normalize, scale_each=True)\n",
    "        # Make vertical grid from image tensor\n",
    "        nrows = int(np.sqrt(num_images))\n",
    "        grid = vutils.make_grid(\n",
    "            images, nrow=nrows, normalize=True, scale_each=True)\n",
    "\n",
    "        # Add horizontal images to tensorboard\n",
    "        self.writer.add_image(img_name, horizontal_grid, step)\n",
    "\n",
    "        # Save plots\n",
    "        self.save_torch_images(horizontal_grid, grid, epoch, n_batch)\n",
    "\n",
    "    def save_torch_images(self, horizontal_grid, grid, epoch, n_batch, plot_horizontal=True):\n",
    "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "\n",
    "        # Plot and save horizontal\n",
    "        fig = plt.figure(figsize=(16, 16))\n",
    "        plt.imshow(np.moveaxis(horizontal_grid.numpy(), 0, -1))\n",
    "        plt.axis('off')\n",
    "        if plot_horizontal:\n",
    "            display.display(plt.gcf())\n",
    "        self._save_images(fig, epoch, n_batch, 'hori')\n",
    "        plt.close()\n",
    "\n",
    "        # Save squared\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(np.moveaxis(grid.numpy(), 0, -1))\n",
    "        plt.axis('off')\n",
    "        self._save_images(fig, epoch, n_batch)\n",
    "        plt.close()\n",
    "\n",
    "    def _save_images(self, fig, epoch, n_batch, comment=''):\n",
    "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "        fig.savefig('{}/{}_epoch_{}_batch_{}.png'.format(out_dir,\n",
    "                                                         comment, epoch, n_batch))\n",
    "\n",
    "    def display_status(self, epoch, num_epochs, n_batch, num_batches, d_error, g_error, d_pred_real, d_pred_fake):\n",
    "        \n",
    "        # var_class = torch.autograd.variable.Variable\n",
    "        if isinstance(d_error, torch.autograd.Variable):\n",
    "            d_error = d_error.data.cpu().numpy()\n",
    "        if isinstance(g_error, torch.autograd.Variable):\n",
    "            g_error = g_error.data.cpu().numpy()\n",
    "        if isinstance(d_pred_real, torch.autograd.Variable):\n",
    "            d_pred_real = d_pred_real.data\n",
    "        if isinstance(d_pred_fake, torch.autograd.Variable):\n",
    "            d_pred_fake = d_pred_fake.data\n",
    "        \n",
    "        \n",
    "        print('Epoch: [{}/{}], Batch Num: [{}/{}]'.format(\n",
    "            epoch,num_epochs, n_batch, num_batches)\n",
    "             )\n",
    "        print('Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(d_error, g_error))\n",
    "        print('D(x): {:.4f}, D(G(z)): {:.4f}'.format(d_pred_real.mean(), d_pred_fake.mean()))\n",
    "\n",
    "    def save_models(self, generator, discriminator, epoch):\n",
    "        out_dir = './data/models/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "        torch.save(generator.state_dict(),\n",
    "                   '{}/G_epoch_{}'.format(out_dir, epoch))\n",
    "        torch.save(discriminator.state_dict(),\n",
    "                   '{}/D_epoch_{}'.format(out_dir, epoch))\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "    # Private Functionality\n",
    "\n",
    "    @staticmethod\n",
    "    def _step(epoch, n_batch, num_batches):\n",
    "        return epoch * num_batches + n_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_dir(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla GAN for MNIST dataset\n",
    "\n",
    "Next we'll download the MNIST dataset as a small dataset on which we can get things running quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "DATA_FOLDER = './torch_data/VGAN/MNIST'\n",
    "def mnist_data():\n",
    "    compose = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "    out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
    "    return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "\n",
    "# Load Dataset and attach a DataLoader\n",
    "\n",
    "data = mnist_data()\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The generator in a GAN is the model\n",
    "you want to help achieve high performance.\n",
    "A generator generates different objects because of the random noise sample.\n",
    "If we make small changes to the noise, we should be able to see corresponding\n",
    "small changes to the output. The generator is driven by a noise vector\n",
    "sampled from a latent space (the domain of $p_z$) and transforms that\n",
    "noise sample into an element of the domain of\n",
    "$p_{data}$.\n",
    "\n",
    "<img src=\"figures/Generator.jpg\" title=\"Generator\" style=\"width: 640px;\" />\n",
    "\n",
    "The generator model can be practically anything that has the right input\n",
    "and output tensor shapes. The \"vanilla\" GAN is the simplest GAN network architecture.\n",
    "Here is the structure of a simple vanilla GAN generator using only\n",
    "fully connected layers:\n",
    "\n",
    "<img src=\"figures/VanillaGAN-Gen.png\" title=\"Generator model\" style=\"width: 640px;\" />\n",
    "\n",
    "And here is sample code for the model's PyTorch Module. Note that since we\n",
    "normalize the real-valued input data to the range [-1,1], to limit the generator to the\n",
    "same range, we use a hyperbolic tangent activation at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer generative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        n_features = 100\n",
    "        n_out = 784\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(1024, n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "# Function to create noise samples for the generator's input\n",
    "\n",
    "def noise(size):\n",
    "    n = torch.randn(size, 100)\n",
    "    if torch.cuda.is_available(): return n.cuda() \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator is a type of classifier, but it is just to classify its input as\n",
    "real or fake.\n",
    "When a fake sample from the generator is given, it should ouptut 0 for fake:\n",
    "\n",
    "<img src=\"figures/DiscriminatorFake.png\" title=\"Discriminator-1\" style=\"width: 640px;\" />\n",
    "\n",
    "On the other hand, if the input is real, it shoudl output 1 for real:\n",
    "\n",
    "<img src=\"figures/DiscriminatorReal.jpg\" title=\"Discriminator-2\" style=\"width: 640px;\" />\n",
    "\n",
    "We'll use the following simple discriminator structure:\n",
    "\n",
    "<img src=\"figures/VanillaGAN-Dis.png\" title=\"VanillaGAN Discriminator\" style=\"width: 640px;\" />\n",
    "\n",
    "Here is the Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer discriminative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        n_features = 784\n",
    "        n_out = 1\n",
    "        \n",
    "        self.hidden0 = nn.Sequential( \n",
    "            nn.Linear(n_features, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(256, n_out),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "def images_to_vectors(images):\n",
    "    return images.view(images.size(0), 784)\n",
    "\n",
    "def vectors_to_images(vectors):\n",
    "    return vectors.view(vectors.size(0), 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the modules\n",
    "\n",
    "Let's create an instance of the generator and discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = DiscriminatorNet()\n",
    "generator = GeneratorNet()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    discriminator.cuda()\n",
    "    generator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the optimizers\n",
    "\n",
    "The optimization is a min-max game.\n",
    "The generator wants to minimize the objective function, whereas the discriminator wants to maximize the same objective function.\n",
    "The discriminator's loss function is binary cross entropy:\n",
    "\n",
    "$$\\mathcal{L}_D = \\max_D\\mathcal{L}(D;G)=\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_x{z}}[\\log (1-D(G(z))]$$\n",
    "\n",
    "The generator does't affect the first term of $\\mathcal{L}_D$, so its goal\n",
    "is a bit simpler, to minimize the second term of $\\mathcal{L}_D$:\n",
    "\n",
    "$$\\mathcal{L}_G = \\min_G\\mathcal{L}(G;D)=\\mathbb{E}_{z \\sim p_x{z}}[\\log (D(G(z))]$$\n",
    "\n",
    "Putting these together we have\n",
    "\n",
    "$$\\min_G\\max_D\\mathcal{L}(D;G)=\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_x{z}}[\\log (1-D(G(z))]$$\n",
    "\n",
    "Here is a diagram of the objective function:\n",
    "\n",
    "<img src=\"figures/GanObjectivefunction.png\" title=\"min-max optimization\" style=\"width: 640px;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: why not select a very strong discriminator?\n",
    "\n",
    "If we have a discriminator that is far superior to the generator, it will quickly determine that all the fake examples are 100% fake with\n",
    "high confidence. That will not be very useful for the generator, which needs a signal to tell it how to make its samples look less fake.\n",
    "On the other hand, if we have a generator that is far superior to the discriminator, we will get predictions indicating that both the real and\n",
    "generated samples are equally likely to be real or fake. This is actually the end goal: a perfect generator. But we are unlikely to obtain a\n",
    "perfect generator, and if the generator is not yet perfect, it is important to keep the generator and discriminator both improving together,\n",
    "with similar \"skill levels\" from the beginning. Since the discriminator has an \"easier\" job than the generator, it will be difficult to keep\n",
    "the competition balanced. We will talk about some solutions to this problem, such as WGANs, later.\n",
    "\n",
    "OK, back to the optimizers.\n",
    "From Goodfellow et al. (2014), the two networks are trained in an alternating fashion.\n",
    "So it will be straightforward to have a separate optimizer for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "\n",
    "# Loss function\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# How many epochs to train for\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "# Number of steps to apply to the discriminator for each step of the generator (1 in Goodfellow et al.)\n",
    "\n",
    "d_steps = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The targets for the discriminator may be 0 or 1 depending on whether we're giving it\n",
    "real or fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = torch.ones(size, 1)\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data\n",
    "\n",
    "def fake_data_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = torch.zeros(size, 1)\n",
    "    if torch.cuda.is_available(): return data.cuda()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function for a single step for the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Propagate real data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    error_real = loss(prediction_real, real_data_target(real_data.size(0)))\n",
    "    error_real.backward()\n",
    "\n",
    "    # Propagate fake data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    error_fake = loss(prediction_fake, fake_data_target(real_data.size(0)))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    # Take a step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error_real + error_fake, prediction_real, prediction_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a function for a single step of the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(optimizer, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Propagate the fake data through the discriminator and backpropagate.\n",
    "    # Note that since we want the generator to output something that gets\n",
    "    # the discriminator to output a 1, we use the real data target here.\n",
    "    prediction = discriminator(fake_data)\n",
    "    error = loss(prediction, real_data_target(prediction.size(0)))\n",
    "    error.backward()\n",
    "    \n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test noise samples\n",
    "\n",
    "Let's generate some noise vectors to use as inputs to the generator.\n",
    "We'll use these samples repeatedly to see the evolution of the generator\n",
    "over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 16\n",
    "test_noise = noise(num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training\n",
    "\n",
    "Now let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(model_name='VGAN', data_name='MNIST')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, (real_batch,_) in enumerate(data_loader):\n",
    "\n",
    "        # Train discriminator on a real batch and a fake batch\n",
    "        \n",
    "        real_data = images_to_vectors(real_batch)\n",
    "        if torch.cuda.is_available(): real_data = real_data.cuda()\n",
    "        fake_data = generator(noise(real_data.size(0))).detach()\n",
    "        d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer,\n",
    "                                                                real_data, fake_data)\n",
    "        \n",
    "        # Train generator\n",
    "\n",
    "        fake_data = generator(noise(real_batch.size(0)))\n",
    "        g_error = train_generator(g_optimizer, fake_data)\n",
    "        \n",
    "        # Log errors and display progress\n",
    "\n",
    "        logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
    "        if (n_batch) % 100 == 0:\n",
    "            display.clear_output(True)\n",
    "            # Display Images\n",
    "            test_images = vectors_to_images(generator(test_noise)).data.cpu()\n",
    "            logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches);\n",
    "            # Display status Logs\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, n_batch, num_batches,\n",
    "                d_error, g_error, d_pred_real, d_pred_fake\n",
    "            )\n",
    "            \n",
    "        # Save model checkpoints\n",
    "        logger.save_models(generator, discriminator, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
